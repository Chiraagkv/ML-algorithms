{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ENDyzM5gfliK"
      ],
      "authorship_tag": "ABX9TyN5oTz2NoPfjBsJdIY6cIJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chiraagkv/ML-algorithms/blob/main/Theoretical_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📚 Theoretical Machine Learning**"
      ],
      "metadata": {
        "id": "y8uG6q2mnVJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Learning**\n",
        "1. Linear Regression"
      ],
      "metadata": {
        "id": "W16HBxlrDAL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression In One Variable**"
      ],
      "metadata": {
        "id": "TxjmIidFJhGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Terminology\n",
        "* $x_i$: $i^{th}$ input variable\n",
        "* $y_i$: $i^{th}$ output variable\n",
        "* $m$: upper limit of $i$\n",
        "* $h(x)$: Hypothesis function\n",
        "* $ŷ$: Predicted output of $h(x)$\n",
        "* $w$: weight\n",
        "* $b$: bias\n",
        "\n"
      ],
      "metadata": {
        "id": "lODKQ_D-Jlpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working Principle\n",
        "### 1. **Hypothesis Function h(x)**\n",
        "  $$h_{w,b}(x)= wx + b$$\n",
        "  * This tries to predict a $y$ for a given $x$\n",
        "\n",
        "### 2. **Cost Function J(w,b)**\n",
        "  * Finds the error in the model\n",
        "  * MSE: $$J(w,b) = \\frac{\\sum_{i=1}^n\\ (ŷ_i - y_i)^2}{2n}$$\n",
        "    * Divided by $2n$ for derivative purposes.\n",
        "\n",
        "### 3. **Gradient Descent**\n",
        "* **Working Principle**:\n",
        "  1. Plot the graph of J vs w (assume b is a constant for now)\n",
        "  2. at the current value of w, find derivative of the J function at that coordinate. Now, negative of the derivative is the path to be taken to reach the minima.\n",
        "  3. alpha is **learning rate** that makes sure that the model does not move too much towards the minima that it overshoots past it.\n",
        "  4. Do same for b also SIMULTANEOUSLY\n",
        "$$w = w - α \\frac{∂}{∂w}J(w,b)\\\\\n",
        "b = b - α \\frac{∂}{∂b}J(w,b)$$\n",
        "\n",
        "### **4. Putting this together**\n",
        "Repeat till minima reached:\n",
        "  1. find $ŷ$ values for your training data with whatever $h(x)$ you have.\n",
        "  2. Find the cost $J(w,b)$ for the present $h(x)$\n",
        "  3. Update the $w$ and $b$ values based on Gradient Descent."
      ],
      "metadata": {
        "id": "Ap3cTIueSkC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example**\n",
        "#### Given:\n",
        "1. h(x) = 2x + 3\n",
        "2. X = [1, 2, 3]\n",
        "3. Y = [3, 5, 7]\n",
        "4. J is MSE: $$J(w,b) = \\frac{\\sum_{i=1}^n\\ (wx_i + b - y_i)^2}{2n}$$\n",
        "5. Calculated Partial Derivatives: $$\\frac{∂}{∂w}J(w, b) = \\frac{\\sum_{i=1}^n\\ (wx_i + b - y_i).x_i}{n}\\\\\n",
        "\\frac{∂}{∂b}J(w, b) = \\frac{\\sum_{i=1}^n\\ (wx_i + b - y_i)}{n}$$\n",
        "6. α = 0.1\n",
        "\n",
        "#### Working:\n",
        "* Y predicted = [5, 7, 9]\n",
        "* J(2, 3) = 2\n",
        "* $w = w - α(\\frac{2×6}{3})$ and $b = b - α(\\frac{2}{3})$\n",
        "* Now, w = 1.6 and b = 2.933\n",
        "* J(1.6, 2.933) = 0.6955 (much much lesser)\n"
      ],
      "metadata": {
        "id": "ENDyzM5gfliK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def hypothesis(w, b, X):\n",
        "  return [w*i+b for i in X]\n",
        "\n",
        "def mse_modded(Y, Y_pred): # modded because of the 1/2\n",
        "  ans = 0\n",
        "  for i in range(len(Y)):\n",
        "    ans += (Y[i] - Y_pred[i])**2\n",
        "  return ans/(2*len(Y))\n",
        "\n",
        "def gradient_descent(X, Y, lr, w, b):\n",
        "  w_correction = lr*(sum([(hypothesis(w, b, X)[i] - Y[i])*X[i] for i in range(len(X))])/len(X))\n",
        "  b_correction = lr*(sum([(hypothesis(w, b, X)[i] - Y[i]) for i in range(len(X))])/len(X))\n",
        "  w -= w_correction\n",
        "  b -= b_correction\n",
        "  return w, b\n",
        "\n",
        "def uni_linear_regression(X, Y, epochs=5, lr=0.01):\n",
        "  w = random.random()\n",
        "  b = random.random()\n",
        "  for i in range(epochs):\n",
        "    Y_pred = hypothesis(w, b, X)\n",
        "    cost = mse_modded(Y, Y_pred)\n",
        "    w, b = gradient_descent(X, Y, lr, w, b)\n",
        "    print(\"Present cost:\", cost)"
      ],
      "metadata": {
        "id": "WHI6L35PuzDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [3, 5, 7, 9, 11]\n",
        "uni_linear_regression(X, Y, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7d1tOs-ywJH",
        "outputId": "954c1e57-79b9-4237-ef1d-aded000bf244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Present cost: 7.663793587397455\n",
            "Present cost: 5.959743670973275\n",
            "Present cost: 4.63504555633149\n",
            "Present cost: 3.605247249968433\n",
            "Present cost: 2.804697621183241\n",
            "Present cost: 2.182360918361637\n",
            "Present cost: 1.6985630655786814\n",
            "Present cost: 1.3224623048196285\n",
            "Present cost: 1.0300829286242839\n",
            "Present cost: 0.8027867468973706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multiple Linear Regression**\n",
        "* NOT multivariate regression (that is predicting multiple output variables.)"
      ],
      "metadata": {
        "id": "_y-EXRamy4YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notation Update**\n",
        "* $X_j$ is the $j^{th}$ feature\n",
        "* $X^{(i)}$ is the $i^{th}$ sample. It is a 1 D matrix (vector)"
      ],
      "metadata": {
        "id": "Ey_qkRTJTKx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working Principle\n",
        "* W is the Weights Matrix of shape (1, n) where n is the number of features.\n",
        "* X is a sample of shape (1, n)\n",
        "* W, X are in capital because they are vectors\n",
        "### **Hypothesis Function h(x)**\n",
        "$$h_{(W,\\ b)}(X) = W_1X_1 + W_2X_2 +...+W_nX_n + b\\\\\n",
        "⇒ h_{(W,\\ b)}(X) = W.X + B\n",
        "$$\n",
        "\n",
        "### **Cost function J(W, b)**\n",
        "* Pretty much the same, except w → W\n",
        "\n",
        "### **Gradient Descent**\n",
        "$$w_j = w_j - α\\frac{∂}{∂w_j}J(W, b)\\ \\ ∀\\ \\ 0 < j ≤ n\\\\\n",
        "b = b - α\\frac{∂}{∂b}J(W, b)$$\n"
      ],
      "metadata": {
        "id": "jqLliuunVD1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def h(X, W, b):\n",
        "  return X@W + b\n",
        "\n",
        "def loss(Y, Y_pred):\n",
        "  return np.sum((Y - Y_pred)**2)/(len(Y)*2)\n",
        "\n",
        "def gradient_descent(W, X, Y, b, lr):\n",
        "  m = len(X)\n",
        "  n = len(X[0])\n",
        "  Y_pred = h(X, W, b)\n",
        "  W_correction = lr * (X.T@(Y_pred-Y))/m #lr * np.sum(np.multiply(np.reshape(Y_pred - Y, (1, len(Y))), X), axis=0)/len(X)\n",
        "  b_correction = lr * np.sum(Y_pred - Y)/m\n",
        "  W -= W_correction\n",
        "  b -= b_correction\n",
        "  return W, b\n",
        "\n",
        "def linear_regression(X, Y, epochs=5, lr=0.01, decay=0):\n",
        "  n = len(X[0])\n",
        "  W = np.random.rand(n)\n",
        "  b = 0\n",
        "  for i in range(epochs):\n",
        "    lr = lr / (1 + decay*i)\n",
        "    Y_pred = h(X, W, b)\n",
        "    cost = loss(Y, Y_pred)\n",
        "    W, b = gradient_descent(W, X, Y, b, lr)\n",
        "  print(f\"Cost {i}: \", cost)\n",
        "  return W, b"
      ],
      "metadata": {
        "id": "ukbHMCM9VqcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(-10, 10, (100, 5))\n",
        "\n",
        "Y = np.sum(X, axis=1) * 2 + 3\n",
        "W, b = linear_regression(X, Y, epochs=5000, lr=0.01, decay=0)\n",
        "\n",
        "print(\"Learned Weights:\", W)\n",
        "print(\"Learned Bias:\", b) # As close to perfection as possible. All weights are 2, bias is basically 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeHbVKMrum_r",
        "outputId": "987bd99e-fea8-4b07-f5df-c61959ca37c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost 4999:  2.935321846043439e-28\n",
            "Learned Weights: [2. 2. 2. 2. 2.]\n",
            "Learned Bias: 2.999999999999975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practical Tips for Linear Regression**"
      ],
      "metadata": {
        "id": "RiSk5dQzNcWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Feature Scaling**\n",
        "* Convert all features to a certain range to help the model converge faster\n",
        "1. Divide all values with the largest one\n",
        "2. find average, subtract it from all and then divide everything by maximum absolute value of the data. (Mean Normalization)\n",
        "3. find mean μ and std. deviation σ\n",
        "apply the transformation: $x_i = \\frac{x_i - μ_i}{σ_i}$ (Z-Score Normalization)\n"
      ],
      "metadata": {
        "id": "VmWM38DCK8Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking If Gradient Descent is working**\n",
        "* If J(W, b) vs epochs curve should always be decreasing. If it increases, there is something wrong with the learning rate.\n",
        "* Test for convergence: Find a acceptably small value of error (ϵ) and as soon as J(w, b) ≤ ϵ, stop it\n",
        "\n",
        "#### **Choosing a Good Learning Rate**\n",
        "* with a small enough α, the model should constant converge. So if it starts oscillation, decrease the lr and try again\n",
        "* 0.001,0.003, 0.01, 0.03, 0.1, 0.3, 1\n",
        "are the go-to\n",
        "\n",
        "#### **Feature Engineering**\n",
        "* We can manipulate whatever features we have to create better features.\n",
        "* example: If house price ∝ area and the data has 2 features: length, breadth. We create a new one which gives area"
      ],
      "metadata": {
        "id": "KUBEWga_M-sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Polynomial Regression**"
      ],
      "metadata": {
        "id": "Ku7qpD08QNfR"
      }
    }
  ]
}